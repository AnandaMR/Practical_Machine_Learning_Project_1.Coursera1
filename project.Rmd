---
title: "Physical Activity Prediction Analysis"
author: "Ananda Rivas"
date: "`r Sys.Date()`"
output: html_document
---

```{r configuration, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE)
```

## Introduction
This analysis aims to predict how six participants performed weightlifting exercises, using data from accelerometers on the belt, forearm, arm, and dumbbell. The target variable is `classe`, which indicates how the exercise was performed.

The data was obtained from the *Weightlifting Exercise Dataset*. It will be explored, cleaned, and modeled to achieve the best possible prediction.

The data for this project come from this source:  
[http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har](http://web.archive.org/web/20161224072740/http:/groupware.les.inf.puc-rio.br/har).  
If you use the document you create for this class for any purpose, please cite them as they have been very generous in allowing their data to be used for this kind of assignment.


## Data Loading
We load the necessary libraries and datasets.

```{r load}
library(caret)
library(randomForest)
library(ggplot2)
library(dplyr)
library(corrplot)
library(knitr)
library(tidyverse)
library(reshape2)

# Load the data
train_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
test_url <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

data_train <- read.csv(train_url, na.strings = c("", "NA", "#DIV/0!"))
data_test <- read.csv(test_url, na.strings = c("", "NA", "#DIV/0!"))
```

## Data Exploration
We perform an initial analysis of the structure and missing values in the data set.

```{r scan}
dim(data_train)

# Count missing values
missing_values <- colSums(is.na(data_train))
missing_values <- missing_values[missing_values > 0]

# Convert a data frame
missing_summary <- data.frame(Variable = names(missing_values), Missing = missing_values)

# Summarize variables with the same amount of missing values
missing_summary <- missing_summary %>%
group_by(Missing) %>%
  summarize(Count = n(), Variables = paste(Variable, collapse = ", ")) %>%
  arrange(desc(Missing))

# Display the wired table
missing_summary %>%
  kable(caption = "Summary of Variables with Missing Values", format = "pipe", col.names = c("Number of Missing Values", "Number of Variables", "Variable Names"))
```
The dataset has 19,622 observations and 160 variables. This means that the dimensions are quite large, which can influence decisions about handling missing values.

In this analysis, we have observed that all variables in the dataset that have missing data have more than 97.9% missing values, meaning that virtually all of the information in each column is missing.

It was decided to remove the columns with more than 90% missing data because these variables contained an excessive amount of incomplete information, which compromised their usefulness in the analysis. Imputation of these data using methods such as KNN or MICE did not prove to be adequate in this case, since missing values in more than 80% of the observations in a column could lead to inaccurate imputations and distort the underlying patterns of the dataset. Additionally, imputing values in columns with so little data available could introduce noise into the model and reduce the reliability of the predictions. Since the dataset has a total of 160 variables, removing these columns did not significantly affect the analysis, and the most complete and representative variables were kept for model training, ensuring a more reliable and consistent dataset.

### Data Cleaning
- Columns with more than 90% missing values were removed.
- Irrelevant columns such as `X`, `user_name`, `raw_timestamp_part_1`, etc. were also excluded.

```{r cleanup}
# Remove columns with many NAs
threshold <- 0.9 * nrow(data_train)
data_train <- data_train[, colSums(is.na(data_train)) < threshold]

# Remove irrelevant columns
data_train <- data_train %>% select(-c(X, user_name, raw_timestamp_part_1, raw_timestamp_part_2, cvtd_timestamp))

dim(data_train)
```

### Correlation Matrix

The correlation matrix helps to visualize relationships between variables. This is useful for detecting multicollinearity or redundancy.
```{r}
# Correlation of numeric variables
cor_data <- cor(data_train[, sapply(data_train, is.numeric)])
# Create the correlation plot
corrplot(cor_data,
    method = "circle", # Display method
    type = "upper", # Only show the top part
    order = "hclust", # Sort by hierarchical clustering
    tl.cex = 0.5, # Reduce the size of the labels (you can adjust it smaller if necessary)
    tl.col = "black", # Text color
    tl.srt = 90, # Rotate the names (45 degrees)
    diag = FALSE, # Remove the diagonal
    mar = c(1, 1, 2, 1), # Adjust margins
    col = colorRampPalette(c("palegreen", "lightblue", "plum"))(200) # Using a color palette custom
    )
```

## Model Selection
Initially, **Random Forest** was considered, but due to significant overfitting, it was replaced with **Gradient Boosting Machine (GBM)**, which generally provides better generalization performance. GBM was chosen for its ability to minimize bias and variance efficiently.

We will split the data into training (75%) and test (25%) and apply **cross-validation**.

```{r split}
set.seed(123)
inTrain <- createDataPartition(data_train$classe, p = 0.75, list = FALSE)
train_set <- data_train[inTrain, ]
test_set <- data_train[-inTrain, ]
```

## Model Training
The Gradient Boosting Machine (GBM) model was trained using 10-fold cross-validation to optimize hyperparameters and improve accuracy. The key hyperparameters that were tuned include:

- `n.trees = c(50, 100)`

- `interaction.depth = c(3, 5)`

- `shrinkage = c(0.01, 0.1)`

- `n.minobsinnode = 10`

```{r training, cache=TRUE}
set.seed(123)
ctrl <- trainControl(method = "cv", number = 10)

rf_model <- train(
  classe ~ ., 
  data = train_set, 
  method = "gbm",
  trControl = ctrl,
  tuneGrid = expand.grid(interaction.depth = c(3, 5), 
                         n.trees = c(50, 100), 
                         shrinkage = c(0.01, 0.1), 
                         n.minobsinnode = 10),
  verbose = FALSE
)

print(rf_model)

#Print the final model to examine chosen tuning parameters/features
rf_model$finalModel
```

The model achieved an impressive accuracy of **99.7%**, and the Kappa value indicates that the predictions align very well with the actual classes. The final model includes 100 iterations and uses 54 predictors, with 53 showing non-zero influence on the model.

This final configuration, with `n.trees = 100`, `interaction.depth = 5`, and `shrinkage = 0.1`, produced the best results in terms of accuracy and prediction alignment with the true classes.



### Expected Out-of-Sample Error
To estimate the model's generalization ability, the out-of-sample error is calculated as:

\[
\text{Expected Error} = 1 - \text{Accuracy}
\]

Given that the lowest observed accuracy during cross-validation was **99.39%**, the estimated out-of-sample error is **0.61%**. This confirms that the model is robust and well-suited for the classification task, successfully addressing the overfitting issue encountered with the initial approach.


## Model Evaluation
### Confusion Matrix
Once the model is trained, we test it on the test set. The confusion matrix allows us to evaluate the accuracy and quality of the model in predicting the classes of the exercises.

```{r evaluation}
predictions <- predict(rf_model, newdata = test_set)
levels(predictions)
levels(test_set$classe)
test_set$classe <- as.factor(test_set$classe)

predictions <- factor(predictions, levels = levels(test_set$classe))
conf_matrix <- confusionMatrix(predictions, test_set$classe)
conf_matrix
```

The **GBM model** demonstrated exceptional classification performance, achieving an overall accuracy of **99.41%**. The high sensitivity and specificity across all classes indicate its strong ability to correctly identify each category while minimizing false positives. The **Kappa statistic of 0.9925** further supports the model's robustness and agreement with actual class labels, making it a highly reliable classifier for this task.

## Overfitting Evaluation
To determine whether the model is overfitting, we compare the accuracy on the training set and the test set. If the accuracy on the training set is significantly higher than on the test set, then overfitting is likely to occur.

```{r}
train_predictions <- predict(rf_model, newdata = train_set)
train_set$classe <- as.factor(train_set$classe)
train_conf_matrix <- confusionMatrix(train_predictions, train_set$classe)
train_conf_matrix
```
To determine whether the model is overfitting, we compare the accuracy on the training set and the test set. If the accuracy on the training set is significantly higher than on the test set, then overfitting is likely to occur.

The model achieved **99.74% accuracy on the training set** and **99.41% accuracy on the test set**, resulting in a minimal accuracy difference of **0.33%**. This suggests that the model does not exhibit significant overfitting, as the difference is quite small. While the near-perfect training accuracy raises some concerns, the model maintains strong generalization to the test set.

## Variable Importance Visualization
We show the most relevant variables in the prediction.

```{r importance, cache=TRUE}
# Training the RandomForest model
data_train$classe <- as.factor(data_train$classe)
rf_model <- randomForest(classe ~ ., data = data_train)

# Get the importance of the variables
imp_vars <- importance(rf_model)

# Convert to a data frame for better handling and sorting
imp_df <- data.frame(Variables = rownames(imp_vars), Importance = imp_vars[, 1])

# Sort by importance (highest to lowest) and select the 10 most important
imp_df_sorted <- imp_df[order(imp_df$Importance, decreasing = TRUE), ]
top_imp_df <- head(imp_df_sorted, 10)
top_imp_df
# Display the most important variables
ggplot(top_imp_df, aes(x = reorder(Variables, Importance), y = Importance, fill = Importance)) +
    geom_bar(stat = "identity", color = "black") + # Black borders to highlight bars
    coord_flip() +
    labs(title = "Importance of Variables",
    x = "Variables",
    y = "Importance") +
        scale_fill_gradientn(colors = c("lightblue", "palegreen", "plum", "lightsalmon")) + # From blue to orange
    theme_minimal(base_size = 12) + # More readable font size
    theme(axis.text.y = element_text(size = 10), # Adjust Y-axis text size
    axis.text.x = element_text(size = 10),
    legend.position = "none") # Hide legend if not needed
```

These results indicate that **belt movement (roll_belt, yaw_belt, pitch_belt)** plays a crucial role in classifying the type of exercise performed. **Forearm and dumbbell motion** are also highly relevant.

Understanding which variables influence predictions helps improve the model and provides insights into physical movement patterns.


## Prediction on the Test Set

```{r prediction}
test_predictions <- predict(rf_model, newdata = data_test)
test_predictions
```

However, due to the lack of true labels in the test set, it was not possible to compute a final confusion matrix for the test predictions. A more comprehensive evaluation would require access to the actual test set labels.

## Conclusion
The Gradient Boosting model was trained and evaluated to classify the data into five categories (A, B, C, D, E). Its generalization capacity was verified by making predictions on the test set, obtaining labels within the expected categories. However, the confusion matrix could not be computed due to the absence of real values for the class variable in the test set, which prevents an accurate evaluation of the model's performance. A more complete validation would require the actual labels.

The model achieved 99.7% accuracy on the test set, with an expected out-of-sample error of 0.3%. Minimal overfitting was observed, but further testing with alternative models and cross-validation strategies could improve generalization.

Belt motion and forearm movement were the most influential variables for classification. However, due to the lack of true labels in the test set, it was not possible to compute a final confusion matrix for the test predictions. A more comprehensive evaluation would require access to the actual test set labels.

Further steps could involve testing other machine learning models, fine-tuning hyperparameters, and collecting additional real-world test labels for more robust validation.
